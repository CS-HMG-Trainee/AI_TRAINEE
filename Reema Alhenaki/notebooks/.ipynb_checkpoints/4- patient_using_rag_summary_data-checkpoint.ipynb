{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed27f8d-c019-4ca7-a9b7-85da428e2750",
   "metadata": {},
   "source": [
    "In this notebook, a rag was built and used summerized data generated by Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a552095d-fd63-4399-bab7-ecf81eb879eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bf902e5-249c-4d61-904e-cff064e20d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes NaN, 0, 0.0, and empty string values from a dictionary\n",
    "def clean_nan_and_zero_values(record: Dict) -> Dict:\n",
    "    \"\"\"Removes NaN, 0, 0.0, and empty string values from a dictionary.\"\"\"\n",
    "    return {k: v for k, v in record.items() if pd.notna(v) and v not in [0, 0.0, \"\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebf8c6de-0944-46fc-838c-1eacfcdee107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts a patient JSON object into a list of chunks\n",
    "def create_patient_chunks(patient_json: Dict) -> List[Dict]:\n",
    "    \"\"\"Converts a patient JSON object into a list of RAG-ready chunks.\"\"\"\n",
    "\n",
    "    chunks = []\n",
    "    pid = patient_json[\"PatientID\"]\n",
    " \n",
    "    def add_chunk(text: str, chunk_type: str, date: str = None):\n",
    "        chunks.append({\n",
    "            \"chunk_id\": str(uuid.uuid4()),\n",
    "            \"text\": text,\n",
    "            \"metadata\": {\n",
    "                \"PatientID\": pid,\n",
    "                \"Type\": chunk_type,\n",
    "                \"Date\": date\n",
    "            }\n",
    "        })\n",
    " \n",
    "    # Split summary into sections (paragraphs)\n",
    "    if \"Summary\" in patient_json:\n",
    "        for paragraph in patient_json[\"Summary\"].split(\"\\n\\n\"):\n",
    "            cleaned = paragraph.strip()\n",
    "            if cleaned:\n",
    "                add_chunk(f\"Patient Summary Section: {cleaned}\", \"SummarySection\")\n",
    " \n",
    "    # PatientInfo\n",
    "\n",
    "    if \"PatientInfo\" in patient_json:\n",
    "        info = clean_nan_and_zero_values(patient_json[\"PatientInfo\"])\n",
    "        if info:\n",
    "            text = \"Patient Info: \" + \", \".join(f\"{k}: {v}\" for k, v in info.items())\n",
    "            add_chunk(text, \"PatientInfo\")\n",
    " \n",
    "    # VitalSigns\n",
    "\n",
    "    for record in patient_json.get(\"VitalSigns\", []):\n",
    "        record = clean_nan_and_zero_values(record)\n",
    "        if record:\n",
    "            text = \"Vital Signs: \" + \", \".join(f\"{k}: {v}\" for k, v in record.items())\n",
    "            add_chunk(text, \"VitalSigns\", record.get(\"CreatedOn\"))\n",
    " \n",
    "    # Appointments\n",
    "    \n",
    "    for record in patient_json.get(\"Appointments\", []):\n",
    "        record = clean_nan_and_zero_values(record)\n",
    "        if record:\n",
    "            text = \"Appointment: \" + \", \".join(f\"{k}: {v}\" for k, v in record.items())\n",
    "            add_chunk(text, \"Appointment\", record.get(\"AppointmentDate\"))\n",
    " \n",
    "    # DoctorOrders\n",
    "\n",
    "    for record in patient_json.get(\"DoctorOrders\", []):\n",
    "        record = clean_nan_and_zero_values(record)\n",
    "        if record:\n",
    "            text = \"Doctor Order: \" + \", \".join(f\"{k}: {v}\" for k, v in record.items())\n",
    "            add_chunk(text, \"DoctorOrders\", record.get(\"ActualOrderDate\"))\n",
    " \n",
    "    return chunks\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6441866b-2e33-48e3-9db8-4841b4121361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Loading JSON file and reading one patient only\n",
    "target_id = 2677554\n",
    " \n",
    "with open(\"C:/Users/reema.alhenaki/Desktop/llama3_Data/data/json/patient_summaries_GEMINI2.json\", \"r\") as f:\n",
    "    all_patients = json.load(f)\n",
    " \n",
    "# Find patient with the matching ID\n",
    "target_patient = next((p for p in all_patients if p.get(\"PatientID\") == target_id), None)\n",
    " \n",
    "if target_patient:\n",
    "    chunks = create_patient_chunks(target_patient)\n",
    "    texts = [c[\"text\"] for c in chunks]\n",
    "    metas = [c[\"metadata\"] for c in chunks]\n",
    "else:\n",
    "    print(f\"Patient with ID {target_id} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6608a46f-7d68-4774-a227-0a28f07be673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Embed and Build FAISS Index \n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(texts, show_progress_bar=True)\n",
    " \n",
    "dimension = embeddings[0].shape[0]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37830809-dba5-4799-9d0c-887a27ec0bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "rag_pipeline = pipeline(\"text-generation\", model=\"tiiuae/falcon-rw-1b\", device=0)  # or smaller model on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aca82fe5-ae2d-4eeb-ab72-cad54c4045f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Define RAG Query Function \n",
    "rag_pipeline = pipeline(\"text-generation\", model=\"tiiuae/falcon-rw-1b\", device=0)  # or smaller model on CPU #Try Temprature 0\n",
    " \n",
    "def query_rag(question: str, top_k: int = 3):\n",
    "    q_embed = model.encode([question])\n",
    "    D, I = index.search(q_embed, top_k)\n",
    "    retrieved_texts = []\n",
    " \n",
    "    # Filter out \"Summary\" chunks using metadata\n",
    "    for i in I[0]:\n",
    "        if metas[i][\"Type\"] != \"Summary\":\n",
    "            retrieved_texts.append(texts[i])\n",
    "    context = \"\\n\".join(retrieved_texts)\n",
    "    prompt = f\"\"\"You are a medical assistant AI. Use the context to answer the question.\n",
    " \n",
    "Context:\n",
    "{context}\n",
    " \n",
    "Question:\n",
    "{question}\n",
    " \n",
    "Answer:\"\"\"\n",
    "    result = rag_pipeline(prompt, max_new_tokens=50, do_sample=True)[0]['generated_text']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e81c2900-b7f1-4057-bebe-6d3009e5966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(question: str, top_k: int = 3):\n",
    "    # Step 1: Embed the query\n",
    "\n",
    "    q_embed = model.encode([question])\n",
    "    D, I = index.search(q_embed, top_k)\n",
    "    retrieved_texts = []\n",
    "\n",
    "    # Filter out \"Summary\" chunks using metadata\n",
    "    for i in I[0]:\n",
    "        if metas[i][\"Type\"] != \"Summary\":\n",
    "            retrieved_texts.append(texts[i])\n",
    "    context = \"\\n\".join(retrieved_texts)\n",
    "\n",
    "    # Step 3: Create a structured, instruction-driven prompt\n",
    "    prompt = f\"\"\"\n",
    "You are a clinical assistant AI. Answer the user's question strictly using the information provided in the context.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Instructions:\n",
    "- Respond only with the relevant values asked in the question\n",
    "- Do not restate the full context\n",
    "- Do not include unrelated medical details that are not mentioned in the question\n",
    "- Only use facts present in the context\n",
    "- Do not guess or hallucinate any values that is not clearly stated\n",
    "- Do not repeat the same values\n",
    "- Be concise and accurate\n",
    "- Only include relevant data that is mentioned in the question do not mention extra data that was not asked\n",
    " \n",
    "Answer:\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ§¾ PROMPT SENT TO LLM:\")\n",
    "    print(prompt)\n",
    "    \n",
    "    # Step 4: Generate the answer\n",
    "    result = rag_pipeline(prompt, max_new_tokens=80, do_sample=False)[0][\"generated_text\"]\n",
    "    \n",
    "    # Step 5: Extract only the answer portion\n",
    "    if \"Answer:\" in result:\n",
    "        answer_part = result.split(\"Answer:\")[1].strip()\n",
    "        answer = answer_part.split(\"Question:\")[0].strip()\n",
    "    else:\n",
    "        answer = result.strip() \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a50892cb-406f-4555-bbf2-abfce29a6b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§¾ PROMPT SENT TO LLM:\n",
      "\n",
      "You are a clinical assistant AI. Answer the user's question strictly using the information provided in the context.\n",
      "Context:\n",
      "Patient Info: RegistrationDate: 30/10/2017, FirstName: Yusuf, MiddleName: Abdullah, LastName: Mubarak, Gender: 1, DateofBirth: 30/10/1963 0:00, NationalityID: SAU, FirstVisit: 30/10/2017 9:42, LastVisit: 1/8/2019 13:05, NoOfVisit: 189, MobileNumber: 555333541, EmailAddress: yusuf@mail.com, BloodGroup: 4, RHFactor: 1, RegisteredDoctor: 152141, EmergencyContactName: AHMAD, EmergencyContactNo: 555333542\n",
      "Vital Signs: PatientID: 2677554, WeightKg: 103.0, HeightCm: 176.0, PulseBeatPerMinute: 85, RespirationBeatPerMinute: 18, BloodPressureLower: 103, BloodPressureHigher: 187, SAO2: 98, CreatedOn: 2018-05-16 14:09:00\n",
      "Patient Summary Section: He has a future appointment (Appointment No. 17107657) scheduled for June 24, 2025, from 1:00 PM to 1:15 PM with Doctor ID 149425 at Clinic ID 50.\n",
      "\n",
      "Question:\n",
      "What is the patient's name?\n",
      "\n",
      "Instructions:\n",
      "- Respond only with the relevant values asked in the question\n",
      "- Do not restate the full context\n",
      "- Do not include unrelated medical details that are not mentioned in the question\n",
      "- Only use facts present in the context\n",
      "- Do not guess or hallucinate any values that is not clearly stated\n",
      "- Do not repeat the same values\n",
      "- Be concise and accurate\n",
      "- Only include relevant data that is mentioned in the question do not mention extra data that was not asked\n",
      "\n",
      "Answer:\n",
      "- Yusuf Mubarak\n",
      "- Abdullah Mubarak\n",
      "- 1\n",
      "- 1\n",
      "- 1\n",
      "- 1\n",
      "- 1\n",
      "- 1\n",
      "- 1\n",
      "- 1\n",
      "- 1\n",
      "- 1\n",
      "- 1\n",
      "- 1\n",
      "- 1\n",
      "- 1\n",
      "- 1\n",
      "- 1\n",
      "- 1\n",
      "- 1\n",
      "- 1\n",
      "- 1\n",
      "- 1\n",
      "- 1\n",
      "- 1\n",
      "-\n"
     ]
    }
   ],
   "source": [
    " #  EXAMPLE QUERY \n",
    "print(query_rag(\"What is the patient's name?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca76e7-abd8-42c2-8a1d-b835e1c39c83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llama3_Data)",
   "language": "python",
   "name": "llama3_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
