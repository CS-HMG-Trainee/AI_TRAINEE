{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf171b2-2ce7-413d-89f3-f3bf4c164a86",
   "metadata": {},
   "source": [
    "In this notebook, a rag was built and used with text data in the format \"feature:value,feature:value,....feature:value\". The summarized data was not used here in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a552095d-fd63-4399-bab7-ecf81eb879eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reema.alhenaki\\AppData\\Local\\anaconda3\\envs\\llama3_Data\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import uuid\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "643636b3-3869-4a86-9ace-34406978fe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Chunking JSON \n",
    "def create_full_chunks(patient_json: Dict) -> List[Dict]:\n",
    "    chunks = []\n",
    "    pid = patient_json[\"PatientID\"]\n",
    " \n",
    "    def format_fields(data: Dict) -> str:\n",
    "        return \", \".join(\n",
    "            f\"{k}: {v}\" for k, v in data.items()\n",
    "            if v not in [0, 0.0, \"\", None]\n",
    "        )\n",
    " \n",
    "    def add_chunk(data: Dict, meta_type: str, date_field: str = None):\n",
    "        text = f\"{meta_type} - \" + format_fields(data)\n",
    "        date = data.get(date_field) if date_field else None\n",
    "        chunks.append({\n",
    "            \"chunk_id\": str(uuid.uuid4()),\n",
    "            \"text\": text,\n",
    "            \"metadata\": {\n",
    "                \"PatientID\": pid,\n",
    "                \"Type\": meta_type,\n",
    "                \"Date\": date\n",
    "            }\n",
    "        })\n",
    " \n",
    "    if \"PatientInfo\" in patient_json:\n",
    "        add_chunk(patient_json[\"PatientInfo\"], \"PatientInfo\", \"DateofBirth\")\n",
    " \n",
    "    for vital in patient_json.get(\"VitalSigns\", []):\n",
    "        add_chunk(vital, \"VitalSigns\", \"CreatedOn\")\n",
    " \n",
    "    for appt in patient_json.get(\"Appointments\", []):\n",
    "        add_chunk(appt, \"Appointment\", \"AppointmentDate\")\n",
    " \n",
    "    for order in patient_json.get(\"DoctorOrders\", []):\n",
    "        add_chunk(order, \"DoctorOrders\", \"ActualOrderDate\")\n",
    " \n",
    "    if \"Summary\" in patient_json:\n",
    "        chunks.append({\n",
    "            \"chunk_id\": str(uuid.uuid4()),\n",
    "            \"text\": f\"Summary - {patient_json['Summary']}\",\n",
    "            \"metadata\": {\n",
    "                \"PatientID\": pid,\n",
    "                \"Type\": \"Summary\",\n",
    "                \"Date\": None\n",
    "            }\n",
    "        })\n",
    " \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6441866b-2e33-48e3-9db8-4841b4121361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data  \n",
    "target_id = 2677554 \n",
    " \n",
    "with open(\"C:/Users/reema.alhenaki/Desktop/llama3_Data/data/json/patient_summaries_GEMINI.json\", \"r\") as f:\n",
    "    all_patients = json.load(f)\n",
    " \n",
    "# Find patient with the matching ID\n",
    "target_patient = next((p for p in all_patients if p.get(\"PatientID\") == target_id), None)\n",
    " \n",
    "if target_patient:\n",
    "    chunks = create_full_chunks(target_patient)\n",
    "    texts = [c[\"text\"] for c in chunks]\n",
    "    metas = [c[\"metadata\"] for c in chunks]\n",
    "else:\n",
    "    print(f\"Patient with ID {target_id} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6608a46f-7d68-4774-a227-0a28f07be673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Embed and Build FAISS Index \n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(texts, show_progress_bar=True)\n",
    " \n",
    "dimension = embeddings[0].shape[0]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37830809-dba5-4799-9d0c-887a27ec0bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "rag_pipeline = pipeline(\"text-generation\", model=\"tiiuae/falcon-rw-1b\", device=0)  # or smaller model on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aca82fe5-ae2d-4eeb-ab72-cad54c4045f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Define RAG Query Function\n",
    "rag_pipeline = pipeline(\"text-generation\", model=\"tiiuae/falcon-rw-1b\", device=0)  # or smaller model on CPU\n",
    " \n",
    "def query_rag(question: str, top_k: int = 3):\n",
    "    q_embed = model.encode([question])\n",
    "    D, I = index.search(q_embed, top_k)\n",
    "    retrieved_texts = []\n",
    " \n",
    "    # Filter out \"Summary\" chunks using metadata\n",
    "    for i in I[0]:\n",
    "        if metas[i][\"Type\"] != \"Summary\":\n",
    "            retrieved_texts.append(texts[i])\n",
    "    context = \"\\n\".join(retrieved_texts)\n",
    "    prompt = f\"\"\"You are a medical assistant AI. Use the context to answer the question.\n",
    " \n",
    "Context:\n",
    "{context}\n",
    " \n",
    "Question:\n",
    "{question}\n",
    " \n",
    "Answer:\"\"\"\n",
    "    result = rag_pipeline(prompt, max_new_tokens=50, do_sample=True)[0]['generated_text']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e81c2900-b7f1-4057-bebe-6d3009e5966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(question: str, top_k: int = 3):\n",
    "    # Step 1: Embed the query\n",
    "\n",
    "    q_embed = model.encode([question])\n",
    "    D, I = index.search(q_embed, top_k)\n",
    "    retrieved_texts = []\n",
    "\n",
    "    # Filter out \"Summary\" chunks using metadata\n",
    "    for i in I[0]:\n",
    "        if metas[i][\"Type\"] != \"Summary\":\n",
    "            retrieved_texts.append(texts[i])\n",
    "    context = \"\\n\".join(retrieved_texts)\n",
    "\n",
    "    # Step 3: Create a structured, instruction-driven prompt\n",
    "    prompt = f\"\"\"\n",
    "You are a clinical assistant AI. Answer the user's question strictly using the information provided in the context.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Instructions:\n",
    "- Respond only with the relevant values asked in the question\n",
    "- Do not restate the full context\n",
    "- Do not include unrelated medical details that are not mentioned in the question\n",
    "- Only use facts present in the context\n",
    "- Do not guess or hallucinate any values that is not clearly stated\n",
    "- Do not repeat the same values\n",
    "- Be concise and accurate\n",
    " \n",
    "Answer:\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ§¾ PROMPT SENT TO LLM:\")\n",
    "    print(prompt)\n",
    "    \n",
    "    # Step 4: Generate the answer\n",
    "    result = rag_pipeline(prompt, max_new_tokens=80, do_sample=False)[0][\"generated_text\"]\n",
    "    \n",
    "    # Step 5: Extract only the answer portion\n",
    "    if \"Answer:\" in result:\n",
    "        answer_part = result.split(\"Answer:\")[1].strip()\n",
    "        answer = answer_part.split(\"Question:\")[0].strip()\n",
    "    else:\n",
    "        answer = result.strip() \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a50892cb-406f-4555-bbf2-abfce29a6b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§¾ PROMPT SENT TO LLM:\n",
      "\n",
      "You are a clinical assistant AI. Answer the user's question strictly using the information provided in the context.\n",
      "Context:\n",
      "PatientInfo - RegistrationDate: 30/10/2017, FirstName: Yusuf, MiddleName: Abdullah, LastName: Mubarak, Gender: 1, DateofBirth: 30/10/1963 0:00, NationalityID: SAU, FirstVisit: 30/10/2017 9:42, LastVisit: 1/8/2019 13:05, NoOfVisit: 189, MobileNumber: 555333541, EmailAddress: yusuf@mail.com, BloodGroup: 4, RHFactor: 1, RegisteredDoctor: 152141, EmergencyContactName: AHMAD, EmergencyContactNo: 555333542\n",
      "VitalSigns - PatientID: 2677554, WeightKg: 103.0, HeightCm: 176.0, PulseBeatPerMinute: 85, RespirationBeatPerMinute: 18, BloodPressureLower: 103, BloodPressureHigher: 187, SAO2: 98, CreatedOn: 2018-05-16 14:09:00\n",
      "Appointment - AppointmentNo: 17107657, AppointmentDate: 2019-08-04, PatientID: 2677554, ClinicID: 50, DoctorID: 149425, StartTime: 2025-06-24 13:00:00, EndTime: 2025-06-24 13:15:00, VisitType: 3, VisitFor: 10\n",
      "\n",
      "Question:\n",
      "What is the patient's name?\n",
      "\n",
      "Instructions:\n",
      "- Respond only with the relevant values asked in the question\n",
      "- Do not restate the full context\n",
      "- Do not include unrelated medical details that are not mentioned in the question\n",
      "- Only use facts present in the context\n",
      "- Do not guess or hallucinate any values that is not clearly stated\n",
      "- Do not repeat the same values\n",
      "- Be concise and accurate\n",
      "\n",
      "Answer:\n",
      "Yusuf Mubarak\n"
     ]
    }
   ],
   "source": [
    " # EXAMPLE QUERY \n",
    "print(query_rag(\"What is the patient's name?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca76e7-abd8-42c2-8a1d-b835e1c39c83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llama3_Data)",
   "language": "python",
   "name": "llama3_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
